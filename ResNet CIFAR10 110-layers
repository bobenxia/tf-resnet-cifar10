{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a demo of a 110-layer residual network applied to the Cifar10 dataset.\n",
    "\n",
    "The 110-layer ResNet is defined as in the [ResNet paper](https://arxiv.org/abs/1512.03385) and the [Identity mapping variant](https://arxiv.org/abs/1603.05027), where the network is mainly composed of three blocks of repeating layers with `[16, 32, 64]` filters, respectively. \n",
    "\n",
    "Per-pixel means are subtracted from each training and test image (see comments below). Four pixels are padded to training images on both sides, and a 32-by-32 crop is taken on each training image (or its horizontal flip). Similar accuracy was observed as in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train[0].shape =  (50000, 32, 32, 3)\n",
      "train[1].shape =  (50000, 10)\n",
      "test[0].shape =  (10000, 32, 32, 3)\n",
      "test[1].shape =  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from cifar10_bin import *\n",
    "from resnet import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "CROP_SIZE = 32\n",
    "dr = DataReader(PATH, one_hot=True)\n",
    "train, test = dr.read_data()\n",
    "print \"train[0].shape = \", train[0].shape\n",
    "print \"train[1].shape = \", train[1].shape\n",
    "print \"test[0].shape = \", test[0].shape\n",
    "print \"test[1].shape = \", test[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for image preprocessing and data augmentation.\n",
    "param = {\"crop_size\": [CROP_SIZE, CROP_SIZE], \n",
    "         \"max_delta\": 63, \n",
    "         \"lower\": 0.2, \n",
    "         \"upper\": 1.8, \n",
    "         \"pad_size\": [4, 4]}\n",
    "\n",
    "# Only `random_crop`, `random_flip_left_right`, `pad_image` (4 pixels on both sides) are applied.\n",
    "proc_img = {\"random_crop\": True, \n",
    "            \"random_flip_left_right\": True, \n",
    "            \"random_brightness\": False, \n",
    "            \"pad_image\": True,\n",
    "            \"random_contrast\": False, \n",
    "            \"per_image_standardization\": False, \n",
    "            \"center_crop\": False}\n",
    "\n",
    "seed = 31415927"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Data Feeder; perform per-pixel mean subtraction on training images and applied on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = DataFeeder(param, BATCH_SIZE, proc_img, seed=seed)\n",
    "\n",
    "train[0] = train[0].reshape((-1, HEIGHT * WIDTH * CHANNELS)).astype(np.float32)\n",
    "test[0] = test[0].reshape((-1, HEIGHT * WIDTH * CHANNELS)).astype(np.float32)\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "train[0] = scaler.fit_transform(train[0]).reshape((-1, HEIGHT, WIDTH, CHANNELS))\n",
    "test[0] = scaler.transform(test[0]).reshape((-1, HEIGHT, WIDTH, CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate is initialized to 0.1 and decayed at the 32K-th and 48K-th iteration by a factor of 0.1, and training is terminated at 64K-th iteration. A learning rate of 0.01 is used in the first 500 iterations to warm up the training as prescribed in the ResNet paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.09375, loss 2.77446\n",
      "step 200, training accuracy 0.335938, loss 2.16106\n",
      "step 400, training accuracy 0.40625, loss 2.01377\n",
      "step 600, training accuracy 0.460938, loss 1.86674\n",
      "step 800, training accuracy 0.554688, loss 1.52851\n",
      "step 1000, training accuracy 0.640625, loss 1.43878\n",
      "step 1200, training accuracy 0.703125, loss 1.20103\n",
      "step 1400, training accuracy 0.6875, loss 1.28496\n",
      "step 1600, training accuracy 0.710938, loss 1.02945\n",
      "step 1800, training accuracy 0.695312, loss 1.15336\n",
      "step 2000, training accuracy 0.742188, loss 1.04644\n",
      "step 2200, training accuracy 0.75, loss 1.01047\n",
      "step 2400, training accuracy 0.765625, loss 0.987982\n",
      "step 2600, training accuracy 0.796875, loss 0.953774\n",
      "step 2800, training accuracy 0.84375, loss 0.809621\n",
      "step 3000, training accuracy 0.8125, loss 0.80742\n",
      "step 3200, training accuracy 0.789062, loss 0.804515\n",
      "step 3400, training accuracy 0.8125, loss 0.755782\n",
      "step 3600, training accuracy 0.851562, loss 0.644111\n",
      "step 3800, training accuracy 0.828125, loss 0.752153\n",
      "step 4000, training accuracy 0.828125, loss 0.759587\n",
      "step 4200, training accuracy 0.851562, loss 0.679349\n",
      "step 4400, training accuracy 0.851562, loss 0.605837\n",
      "step 4600, training accuracy 0.859375, loss 0.62503\n",
      "step 4800, training accuracy 0.90625, loss 0.538674\n",
      "step 5000, training accuracy 0.875, loss 0.621553\n",
      "step 5200, training accuracy 0.84375, loss 0.60646\n",
      "step 5400, training accuracy 0.898438, loss 0.554561\n",
      "step 5600, training accuracy 0.890625, loss 0.539392\n",
      "step 5800, training accuracy 0.875, loss 0.620697\n",
      "step 6000, training accuracy 0.867188, loss 0.602704\n",
      "step 6200, training accuracy 0.898438, loss 0.584246\n",
      "step 6400, training accuracy 0.859375, loss 0.482252\n",
      "step 6600, training accuracy 0.882812, loss 0.646207\n",
      "step 6800, training accuracy 0.867188, loss 0.549295\n",
      "step 7000, training accuracy 0.90625, loss 0.50823\n",
      "step 7200, training accuracy 0.882812, loss 0.600654\n",
      "step 7400, training accuracy 0.898438, loss 0.553393\n",
      "step 7600, training accuracy 0.867188, loss 0.600366\n",
      "step 7800, training accuracy 0.851562, loss 0.593497\n",
      "step 8000, training accuracy 0.882812, loss 0.593103\n",
      "step 8200, training accuracy 0.84375, loss 0.696444\n",
      "step 8400, training accuracy 0.9375, loss 0.467344\n",
      "step 8600, training accuracy 0.929688, loss 0.392927\n",
      "step 8800, training accuracy 0.867188, loss 0.590456\n",
      "step 9000, training accuracy 0.90625, loss 0.478705\n",
      "step 9200, training accuracy 0.914062, loss 0.482544\n",
      "step 9400, training accuracy 0.882812, loss 0.534936\n",
      "step 9600, training accuracy 0.882812, loss 0.550719\n",
      "step 9800, training accuracy 0.945312, loss 0.417786\n",
      "step 10000, training accuracy 0.882812, loss 0.543843\n",
      "step 10200, training accuracy 0.898438, loss 0.480064\n",
      "step 10400, training accuracy 0.90625, loss 0.467693\n",
      "step 10600, training accuracy 0.882812, loss 0.617422\n",
      "step 10800, training accuracy 0.945312, loss 0.398601\n",
      "step 11000, training accuracy 0.867188, loss 0.547202\n",
      "step 11200, training accuracy 0.914062, loss 0.543658\n",
      "step 11400, training accuracy 0.890625, loss 0.491879\n",
      "step 11600, training accuracy 0.882812, loss 0.547185\n",
      "step 11800, training accuracy 0.890625, loss 0.516776\n",
      "step 12000, training accuracy 0.898438, loss 0.490425\n",
      "step 12200, training accuracy 0.921875, loss 0.461525\n",
      "step 12400, training accuracy 0.9375, loss 0.514524\n",
      "step 12600, training accuracy 0.898438, loss 0.485065\n",
      "step 12800, training accuracy 0.90625, loss 0.501502\n",
      "step 13000, training accuracy 0.953125, loss 0.389582\n",
      "step 13200, training accuracy 0.875, loss 0.556605\n",
      "step 13400, training accuracy 0.945312, loss 0.418446\n",
      "step 13600, training accuracy 0.898438, loss 0.496833\n",
      "step 13800, training accuracy 0.914062, loss 0.449903\n",
      "step 14000, training accuracy 0.945312, loss 0.436275\n",
      "step 14200, training accuracy 0.90625, loss 0.510792\n",
      "step 14400, training accuracy 0.859375, loss 0.651582\n",
      "step 14600, training accuracy 0.867188, loss 0.549753\n",
      "step 14800, training accuracy 0.890625, loss 0.513731\n",
      "step 15000, training accuracy 0.898438, loss 0.505041\n",
      "step 15200, training accuracy 0.914062, loss 0.447911\n",
      "step 15400, training accuracy 0.921875, loss 0.46153\n",
      "step 15600, training accuracy 0.882812, loss 0.48024\n",
      "step 15800, training accuracy 0.9375, loss 0.403989\n",
      "step 16000, training accuracy 0.945312, loss 0.436207\n",
      "step 16200, training accuracy 0.875, loss 0.571156\n",
      "step 16400, training accuracy 0.9375, loss 0.443756\n",
      "step 16600, training accuracy 0.960938, loss 0.377733\n",
      "step 16800, training accuracy 0.929688, loss 0.435881\n",
      "step 17000, training accuracy 0.953125, loss 0.443131\n",
      "step 17200, training accuracy 0.929688, loss 0.484089\n",
      "step 17400, training accuracy 0.945312, loss 0.400025\n",
      "step 17600, training accuracy 0.96875, loss 0.371407\n",
      "step 17800, training accuracy 0.9375, loss 0.499607\n",
      "step 18000, training accuracy 0.890625, loss 0.521811\n",
      "step 18200, training accuracy 0.945312, loss 0.391569\n",
      "step 18400, training accuracy 0.90625, loss 0.626267\n",
      "step 18600, training accuracy 0.921875, loss 0.462387\n",
      "step 18800, training accuracy 0.9375, loss 0.386432\n",
      "step 19000, training accuracy 0.9375, loss 0.440007\n",
      "step 19200, training accuracy 0.929688, loss 0.471595\n",
      "step 19400, training accuracy 0.953125, loss 0.397309\n",
      "step 19600, training accuracy 0.953125, loss 0.396398\n",
      "step 19800, training accuracy 0.898438, loss 0.580815\n",
      "step 20000, training accuracy 0.921875, loss 0.424871\n",
      "step 20200, training accuracy 0.945312, loss 0.434863\n",
      "step 20400, training accuracy 0.929688, loss 0.460386\n",
      "step 20600, training accuracy 0.953125, loss 0.439435\n",
      "step 20800, training accuracy 0.914062, loss 0.498842\n",
      "step 21000, training accuracy 0.90625, loss 0.528269\n",
      "step 21200, training accuracy 0.929688, loss 0.489224\n",
      "step 21400, training accuracy 0.953125, loss 0.38866\n",
      "step 21600, training accuracy 0.929688, loss 0.443044\n",
      "step 21800, training accuracy 0.90625, loss 0.535508\n",
      "step 22000, training accuracy 0.914062, loss 0.481734\n",
      "step 22200, training accuracy 0.976562, loss 0.354926\n",
      "step 22400, training accuracy 0.953125, loss 0.454881\n",
      "step 22600, training accuracy 0.953125, loss 0.404141\n",
      "step 22800, training accuracy 0.953125, loss 0.453088\n",
      "step 23000, training accuracy 0.9375, loss 0.514452\n",
      "step 23200, training accuracy 0.898438, loss 0.549039\n",
      "step 23400, training accuracy 0.953125, loss 0.365279\n",
      "step 23600, training accuracy 0.953125, loss 0.4121\n",
      "step 23800, training accuracy 0.960938, loss 0.376007\n",
      "step 24000, training accuracy 0.960938, loss 0.381551\n",
      "step 24200, training accuracy 0.914062, loss 0.61832\n",
      "step 24400, training accuracy 0.921875, loss 0.460177\n",
      "step 24600, training accuracy 0.914062, loss 0.402281\n",
      "step 24800, training accuracy 0.929688, loss 0.485631\n",
      "step 25000, training accuracy 0.945312, loss 0.417111\n",
      "step 25200, training accuracy 0.921875, loss 0.54055\n",
      "step 25400, training accuracy 0.875, loss 0.615618\n",
      "step 25600, training accuracy 0.914062, loss 0.424353\n",
      "step 25800, training accuracy 0.945312, loss 0.456255\n",
      "step 26000, training accuracy 0.914062, loss 0.599309\n",
      "step 26200, training accuracy 0.929688, loss 0.412956\n",
      "step 26400, training accuracy 0.953125, loss 0.398177\n",
      "step 26600, training accuracy 0.90625, loss 0.507728\n",
      "step 26800, training accuracy 0.921875, loss 0.540962\n",
      "step 27000, training accuracy 0.890625, loss 0.515828\n",
      "step 27200, training accuracy 0.929688, loss 0.451752\n",
      "step 27400, training accuracy 0.960938, loss 0.380456\n",
      "step 27600, training accuracy 0.9375, loss 0.469064\n",
      "step 27800, training accuracy 0.921875, loss 0.417897\n",
      "step 28000, training accuracy 0.945312, loss 0.410271\n",
      "step 28200, training accuracy 0.9375, loss 0.450497\n",
      "step 28400, training accuracy 0.914062, loss 0.450495\n",
      "step 28600, training accuracy 0.960938, loss 0.404219\n",
      "step 28800, training accuracy 0.9375, loss 0.452175\n",
      "step 29000, training accuracy 0.9375, loss 0.43642\n",
      "step 29200, training accuracy 0.9375, loss 0.468927\n",
      "step 29400, training accuracy 0.921875, loss 0.497443\n",
      "step 29600, training accuracy 0.960938, loss 0.382664\n",
      "step 29800, training accuracy 0.945312, loss 0.444326\n",
      "step 30000, training accuracy 0.960938, loss 0.375313\n",
      "step 30200, training accuracy 0.953125, loss 0.401099\n",
      "step 30400, training accuracy 0.9375, loss 0.491793\n",
      "step 30600, training accuracy 0.960938, loss 0.40779\n",
      "step 30800, training accuracy 0.960938, loss 0.354915\n",
      "step 31000, training accuracy 0.929688, loss 0.416407\n",
      "step 31200, training accuracy 0.960938, loss 0.445564\n",
      "step 31400, training accuracy 0.945312, loss 0.409058\n",
      "step 31600, training accuracy 0.945312, loss 0.503715\n",
      "step 31800, training accuracy 0.945312, loss 0.495794\n",
      "step 32000, training accuracy 0.914062, loss 0.472762\n",
      "step 32200, training accuracy 0.992188, loss 0.313275\n",
      "step 32400, training accuracy 0.976562, loss 0.31783\n",
      "step 32600, training accuracy 0.984375, loss 0.306042\n",
      "step 32800, training accuracy 0.984375, loss 0.293764\n",
      "step 33000, training accuracy 0.976562, loss 0.313362\n",
      "step 33200, training accuracy 1, loss 0.270722\n",
      "step 33400, training accuracy 0.992188, loss 0.302679\n",
      "step 33600, training accuracy 1, loss 0.280169\n",
      "step 33800, training accuracy 0.953125, loss 0.363097\n",
      "step 34000, training accuracy 0.984375, loss 0.288644\n",
      "step 34200, training accuracy 1, loss 0.268276\n",
      "step 34400, training accuracy 0.96875, loss 0.311615\n",
      "step 34600, training accuracy 0.992188, loss 0.276257\n",
      "step 34800, training accuracy 1, loss 0.250706\n",
      "step 35000, training accuracy 0.984375, loss 0.272127\n",
      "step 35200, training accuracy 1, loss 0.245786\n",
      "step 35400, training accuracy 0.992188, loss 0.254939\n",
      "step 35600, training accuracy 0.992188, loss 0.249405\n",
      "step 35800, training accuracy 0.992188, loss 0.261518\n",
      "step 36000, training accuracy 0.984375, loss 0.281894\n",
      "step 36200, training accuracy 0.976562, loss 0.295298\n",
      "step 36400, training accuracy 0.992188, loss 0.248862\n",
      "step 36600, training accuracy 1, loss 0.230327\n",
      "step 36800, training accuracy 0.992188, loss 0.234963\n",
      "step 37000, training accuracy 1, loss 0.22609\n",
      "step 37200, training accuracy 0.992188, loss 0.243331\n",
      "step 37400, training accuracy 0.984375, loss 0.246004\n",
      "step 37600, training accuracy 0.992188, loss 0.228389\n",
      "step 37800, training accuracy 1, loss 0.222033\n",
      "step 38000, training accuracy 1, loss 0.219755\n",
      "step 38200, training accuracy 0.992188, loss 0.235529\n",
      "step 38400, training accuracy 0.992188, loss 0.240021\n",
      "step 38600, training accuracy 1, loss 0.219306\n",
      "step 38800, training accuracy 1, loss 0.212687\n",
      "step 39000, training accuracy 0.992188, loss 0.230209\n",
      "step 39200, training accuracy 0.992188, loss 0.225465\n",
      "step 39400, training accuracy 1, loss 0.209162\n",
      "step 39600, training accuracy 0.992188, loss 0.216433\n",
      "step 39800, training accuracy 0.984375, loss 0.246626\n",
      "step 40000, training accuracy 0.984375, loss 0.220306\n",
      "step 40200, training accuracy 1, loss 0.200732\n",
      "step 40400, training accuracy 1, loss 0.19949\n",
      "step 40600, training accuracy 1, loss 0.202073\n",
      "step 40800, training accuracy 1, loss 0.199448\n",
      "step 41000, training accuracy 0.992188, loss 0.202006\n",
      "step 41200, training accuracy 1, loss 0.200546\n",
      "step 41400, training accuracy 1, loss 0.189779\n",
      "step 41600, training accuracy 0.992188, loss 0.210009\n",
      "step 41800, training accuracy 0.992188, loss 0.216304\n",
      "step 42000, training accuracy 1, loss 0.190269\n",
      "step 42200, training accuracy 0.992188, loss 0.200927\n",
      "step 42400, training accuracy 0.992188, loss 0.206248\n",
      "step 42600, training accuracy 1, loss 0.185637\n",
      "step 42800, training accuracy 1, loss 0.185705\n",
      "step 43000, training accuracy 1, loss 0.18259\n",
      "step 43200, training accuracy 0.976562, loss 0.211453\n",
      "step 43400, training accuracy 0.992188, loss 0.232561\n",
      "step 43600, training accuracy 1, loss 0.177272\n",
      "step 43800, training accuracy 1, loss 0.185176\n",
      "step 44000, training accuracy 1, loss 0.17711\n",
      "step 44200, training accuracy 1, loss 0.174506\n",
      "step 44400, training accuracy 1, loss 0.180227\n",
      "step 44600, training accuracy 1, loss 0.168908\n",
      "step 44800, training accuracy 1, loss 0.168294\n",
      "step 45000, training accuracy 1, loss 0.166917\n",
      "step 45200, training accuracy 1, loss 0.170558\n",
      "step 45400, training accuracy 0.992188, loss 0.179176\n",
      "step 45600, training accuracy 0.992188, loss 0.187379\n",
      "step 45800, training accuracy 1, loss 0.166522\n",
      "step 46000, training accuracy 0.992188, loss 0.170039\n",
      "step 46200, training accuracy 0.976562, loss 0.218691\n",
      "step 46400, training accuracy 1, loss 0.165729\n",
      "step 46600, training accuracy 1, loss 0.160489\n",
      "step 46800, training accuracy 1, loss 0.158503\n",
      "step 47000, training accuracy 1, loss 0.157576\n",
      "step 47200, training accuracy 1, loss 0.157164\n",
      "step 47400, training accuracy 0.992188, loss 0.170012\n",
      "step 47600, training accuracy 1, loss 0.160137\n",
      "step 47800, training accuracy 1, loss 0.152931\n",
      "step 48000, training accuracy 0.992188, loss 0.163202\n",
      "step 48200, training accuracy 0.992188, loss 0.158012\n",
      "step 48400, training accuracy 1, loss 0.152918\n",
      "step 48600, training accuracy 1, loss 0.15508\n",
      "step 48800, training accuracy 0.992188, loss 0.160634\n",
      "step 49000, training accuracy 1, loss 0.151934\n",
      "step 49200, training accuracy 1, loss 0.150325\n",
      "step 49400, training accuracy 1, loss 0.149566\n",
      "step 49600, training accuracy 1, loss 0.150822\n",
      "step 49800, training accuracy 1, loss 0.162105\n",
      "step 50000, training accuracy 1, loss 0.151767\n",
      "step 50200, training accuracy 1, loss 0.148835\n",
      "step 50400, training accuracy 1, loss 0.149424\n",
      "step 50600, training accuracy 1, loss 0.148438\n",
      "step 50800, training accuracy 1, loss 0.152828\n",
      "step 51000, training accuracy 1, loss 0.147889\n",
      "step 51200, training accuracy 0.992188, loss 0.176161\n",
      "step 51400, training accuracy 1, loss 0.149125\n",
      "step 51600, training accuracy 1, loss 0.1475\n",
      "step 51800, training accuracy 1, loss 0.150214\n",
      "step 52000, training accuracy 1, loss 0.148284\n",
      "step 52200, training accuracy 1, loss 0.147722\n",
      "step 52400, training accuracy 1, loss 0.153373\n",
      "step 52600, training accuracy 1, loss 0.152371\n",
      "step 52800, training accuracy 1, loss 0.149431\n",
      "step 53000, training accuracy 1, loss 0.152081\n",
      "step 53200, training accuracy 0.992188, loss 0.157378\n",
      "step 53400, training accuracy 1, loss 0.150424\n",
      "step 53600, training accuracy 1, loss 0.150683\n",
      "step 53800, training accuracy 1, loss 0.146618\n",
      "step 54000, training accuracy 1, loss 0.146616\n",
      "step 54200, training accuracy 1, loss 0.146632\n",
      "step 54400, training accuracy 1, loss 0.147542\n",
      "step 54600, training accuracy 1, loss 0.147585\n",
      "step 54800, training accuracy 1, loss 0.147105\n",
      "step 55000, training accuracy 1, loss 0.146072\n",
      "step 55200, training accuracy 1, loss 0.146337\n",
      "step 55400, training accuracy 0.992188, loss 0.153254\n",
      "step 55600, training accuracy 1, loss 0.150819\n",
      "step 55800, training accuracy 1, loss 0.145037\n",
      "step 56000, training accuracy 1, loss 0.145092\n",
      "step 56200, training accuracy 1, loss 0.148291\n",
      "step 56400, training accuracy 0.992188, loss 0.16175\n",
      "step 56600, training accuracy 1, loss 0.145892\n",
      "step 56800, training accuracy 1, loss 0.147551\n",
      "step 57000, training accuracy 1, loss 0.145855\n",
      "step 57200, training accuracy 1, loss 0.146507\n",
      "step 57400, training accuracy 1, loss 0.144981\n",
      "step 57600, training accuracy 1, loss 0.14508\n",
      "step 57800, training accuracy 1, loss 0.149749\n",
      "step 58000, training accuracy 1, loss 0.144177\n",
      "step 58200, training accuracy 1, loss 0.144415\n",
      "step 58400, training accuracy 1, loss 0.14689\n",
      "step 58600, training accuracy 1, loss 0.14525\n",
      "step 58800, training accuracy 1, loss 0.145849\n",
      "step 59000, training accuracy 1, loss 0.144314\n",
      "step 59200, training accuracy 1, loss 0.145459\n",
      "step 59400, training accuracy 1, loss 0.146599\n",
      "step 59600, training accuracy 1, loss 0.142863\n",
      "step 59800, training accuracy 1, loss 0.145041\n",
      "step 60000, training accuracy 1, loss 0.143015\n",
      "step 60200, training accuracy 1, loss 0.142983\n",
      "step 60400, training accuracy 1, loss 0.142761\n",
      "step 60600, training accuracy 1, loss 0.142864\n",
      "step 60800, training accuracy 1, loss 0.142927\n",
      "step 61000, training accuracy 1, loss 0.143422\n",
      "step 61200, training accuracy 1, loss 0.14244\n",
      "step 61400, training accuracy 1, loss 0.141825\n",
      "step 61600, training accuracy 1, loss 0.141901\n",
      "step 61800, training accuracy 1, loss 0.143634\n",
      "step 62000, training accuracy 1, loss 0.143397\n",
      "step 62200, training accuracy 1, loss 0.142098\n",
      "step 62400, training accuracy 1, loss 0.141597\n",
      "step 62600, training accuracy 1, loss 0.142209\n",
      "step 62800, training accuracy 1, loss 0.1418\n",
      "step 63000, training accuracy 1, loss 0.141584\n",
      "step 63200, training accuracy 1, loss 0.141346\n",
      "step 63400, training accuracy 1, loss 0.140685\n",
      "step 63600, training accuracy 1, loss 0.14064\n",
      "step 63800, training accuracy 1, loss 0.141475\n"
     ]
    }
   ],
   "source": [
    "INITIAL_LR = 0.1\n",
    "global_step = tf.Variable(0, dtype=tf.float32, trainable=False)\n",
    "lr = tf.cond(tf.less(global_step, 500),\n",
    "              lambda: tf.constant(0.01),\n",
    "              lambda: tf.cond(tf.less(global_step, 32000),\n",
    "                lambda: tf.constant(INITIAL_LR),\n",
    "                lambda: tf.cond(tf.less(global_step, 48000),\n",
    "                  lambda: tf.constant(INITIAL_LR / 10.),\n",
    "                  lambda: tf.constant(INITIAL_LR / 100.))))\n",
    "\n",
    "resnet = ResNet(\"/home/chaoji/Desktop/resnet/train/model\", num_blocks=18, seed=seed)\n",
    "resnet.train(train[0], train[1], df, global_step, lr, 64000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the batch norm population mean and variance from the test images (`test[0].shape[0] == 10000`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.bn_population_estimate(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the predictions (`y_pred.shape == (10000,)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = resnet.test(test[0], 1000, use_population_estimate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9376\n"
     ]
    }
   ],
   "source": [
    "y_true = test[1].argmax(axis=1)\n",
    "test_accuracy = (y_pred == y_true).astype(np.float32).mean()\n",
    "print test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
